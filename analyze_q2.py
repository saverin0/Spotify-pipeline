import os
import logging
import pandas as pd
import statsmodels.api as sm
from sqlalchemy import create_engine
from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime, timedelta
from dotenv import load_dotenv

# ------------------------------------------------------------------------------
# Load environment variables
# ------------------------------------------------------------------------------
load_dotenv()
POSTGRES_CONN_STR = os.getenv("POSTGRES_CONN_STR")
DATA_DIR = os.getenv("DATA_DIR", "./data")
OUTPUT_DIR = os.getenv("OUTPUT_DIR", os.path.join(DATA_DIR, "output"))
os.makedirs(OUTPUT_DIR, exist_ok=True)

# ------------------------------------------------------------------------------
# Configure logging
# ------------------------------------------------------------------------------
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ------------------------------------------------------------------------------
# Function to delete specific old CSV files from OUTPUT_DIR
# ------------------------------------------------------------------------------
def delete_old_outputs(**kwargs):
    """
    Delete old CSV files generated by previous runs.
    Only deletes files created by this DAG: 
      - correlation_matrix.csv
      - regression_results.csv
    """
    try:
        logger.info("Starting task: delete_old_outputs")
        files_to_delete = ["correlation_matrix.csv", "regression_results.csv"]
        deleted_any = False

        for filename in files_to_delete:
            file_path = os.path.join(OUTPUT_DIR, filename)
            if os.path.exists(file_path):
                os.remove(file_path)
                deleted_any = True
                logger.info(f"Deleted old CSV file: {file_path}")

        if not deleted_any:
            logger.info("No CSV files found to delete.")

        logger.info("Old CSV file deletion process complete.")

    except Exception as e:
        logger.error(f"Error deleting old CSV files: {e}")
        raise

# ------------------------------------------------------------------------------
# Function to analyze correlation and regression in chunks
# ------------------------------------------------------------------------------
def analyze_q2(**kwargs):
    """
    Analyze correlation between features and popularity
    and perform regression analysis in a chunk-based approach.
    """
    try:
        logger.info("Starting task: analyze_q2")

        # Ensure POSTGRES_CONN_STR is set
        if not POSTGRES_CONN_STR:
            raise ValueError("POSTGRES_CONN_STR environment variable is not set.")

        # Define relevant features for analysis
        relevant_features = [
            'popularity', 'tempo', 'danceability', 'energy', 'key',
            'loudness', 'speechiness', 'acousticness',
            'instrumentalness', 'liveness', 'valence', 'time_signature'
        ]

        # Read data from the database in chunks
        logger.info("Fetching data from 'raw_tracks' table in chunks...")
        df_tracks_list = []
        chunk_size = 50000  # Adjust as needed

        with create_engine(POSTGRES_CONN_STR).connect() as connection:
            chunk_iter = pd.read_sql_query(
                "SELECT * FROM raw_tracks",
                con=connection,
                chunksize=chunk_size
            )

            # Process each chunk
            for i, chunk in enumerate(chunk_iter, start=1):
                logger.info(f"Processing chunk {i}")
                # Drop rows missing any of the relevant features
                chunk = chunk.dropna(subset=relevant_features, how='any')
                # Verify that all the required columns exist
                missing_features = [col for col in relevant_features if col not in chunk.columns]
                if missing_features:
                    raise ValueError(f"Missing required columns in chunk {i}: {missing_features}")

                # Select only the relevant columns
                chunk = chunk[relevant_features]
                df_tracks_list.append(chunk)

        # Combine all chunks into one DataFrame
        if not df_tracks_list:
            raise ValueError("No data found after reading in chunks. Cannot perform analysis.")

        df_tracks = pd.concat(df_tracks_list, ignore_index=True)
        logger.info(f"Combined DataFrame shape: {df_tracks.shape}")

        # ------------------------------------------------------------------------------
        # Correlation Analysis
        # ------------------------------------------------------------------------------
        logger.info("Calculating correlation matrix...")
        correlation_matrix = df_tracks.corr()
        correlation_csv_path = os.path.join(OUTPUT_DIR, "correlation_matrix.csv")
        correlation_matrix.to_csv(correlation_csv_path)
        logger.info(f"Correlation matrix saved to: {correlation_csv_path}")

        # Uncomment below if you'd like to save the correlation matrix to the database.
        # with create_engine(POSTGRES_CONN_STR).connect() as connection:
        #     correlation_matrix.to_sql('correlation_matrix', connection, if_exists='replace', index=False)

        # ------------------------------------------------------------------------------
        # Regression Analysis
        # ------------------------------------------------------------------------------
        features = [
            'tempo', 'danceability', 'energy', 'key',
            'loudness', 'speechiness', 'acousticness',
            'instrumentalness', 'liveness', 'valence', 'time_signature'
        ]
        target = 'popularity'

        logger.info("Preparing data for regression analysis...")
        df_regression = df_tracks[features + [target]].dropna()
        X = sm.add_constant(df_regression[features])  # Add constant term
        y = df_regression[target]
        logger.info(f"Number of records for regression: {len(X)}")

        logger.info("Performing regression analysis using OLS...")
        model = sm.OLS(y, X).fit()

        logger.info("Saving regression results...")
        regression_results = pd.DataFrame({
            'Feature': X.columns,
            'Coefficient': model.params,
            'P-Value': model.pvalues,
            'Standard Error': model.bse
        })
        regression_csv_path = os.path.join(OUTPUT_DIR, "regression_results.csv")
        regression_results.to_csv(regression_csv_path, index=False)
        logger.info(f"Regression results saved to: {regression_csv_path}")

        # Uncomment below if you'd like to save the regression results to the database.
        # with create_engine(POSTGRES_CONN_STR).connect() as connection:
        #     regression_results.to_sql('regression_results', connection, if_exists='replace', index=False)

        logger.info("Analysis completed successfully.")

    except Exception as e:
        logger.error(f"Error in analyze_q2: {e}")
        raise

# ------------------------------------------------------------------------------
# Airflow DAG Setup
# ------------------------------------------------------------------------------
default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}

with DAG(
    'spotify_analysis_q2',
    default_args=default_args,
    description='Spotify Data Analysis DAG (Chunked) - Correlation and Regression Analysis',
    schedule_interval=None,
    start_date=datetime(2025, 1, 1),
    catchup=False,
) as dag:

    # Task 1: Delete old CSV outputs (only the ones created by this code)
    task_delete_old_outputs = PythonOperator(
        task_id='delete_old_outputs',
        python_callable=delete_old_outputs,
        provide_context=True,
    )

    # Task 2: Analyze Correlation and Regression (Chunked Approach)
    task_analyze_q2 = PythonOperator(
        task_id='analyze_q2',
        python_callable=analyze_q2,
        provide_context=True,
    )

    # Define task dependencies (delete outputs, then analyze)
    task_delete_old_outputs >> task_analyze_q2
